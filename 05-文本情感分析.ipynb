{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 『NLP直播课』Day 5：情感分析预训练模型SKEP\n",
    "\n",
    "本项目将详细全面介绍情感分析任务的两种子任务，句子级情感分析和目标级情感分析。\n",
    "\n",
    "同时演示如何使用情感分析预训练模型SKEP完成以上两种任务，详细介绍预训练模型SKEP及其在 PaddleNLP 的使用方式。\n",
    "\n",
    "本项目主要包括“任务介绍”、“情感分析预训练模型SKEP”、“句子级情感分析”、“目标级情感分析”等四个部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/7a/e6098c8794d7753470071f58b07843824c40ddbabe213eae458d321d2dbe/paddlenlp-2.0.3-py3-none-any.whl (451kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 42kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.1\n",
      "    Uninstalling paddlenlp-2.0.1:\n",
      "      Successfully uninstalled paddlenlp-2.0.1\n",
      "Successfully installed paddlenlp-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Part A. 情感分析任务\n",
    "\n",
    "众所周知，人类自然语言中包含了丰富的情感色彩：表达人的情绪（如悲伤、快乐）、表达人的心情（如倦怠、忧郁）、表达人的喜好（如喜欢、讨厌）、表达人的个性特征和表达人的立场等等。情感分析在商品喜好、消费决策、舆情分析等场景中均有应用。利用机器自动分析这些情感倾向，不但有助于帮助企业了解消费者对其产品的感受，为产品改进提供依据；同时还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
    "\n",
    "被人们所熟知的情感分析任务是将一段文本分类，如分为情感极性为**正向**、**负向**、**其他**的三分类问题：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b630901b397e4e7a8e78ab1d306dfa1fc070d91015a64ef0b8d590aaa8cfde14\" width=\"600\" ></center>\n",
    "<br><center>情感分析任务</center></br>\n",
    "\n",
    "- **正向：** 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "- **负向：** 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "- **其他：** 其他类型的情感。\n",
    "\n",
    "实际上，以上熟悉的情感分析任务是**句子级情感分析任务**。\n",
    "\n",
    "\n",
    "情感分析任务还可以进一步分为**句子级情感分析**、**目标级情感分析**等任务。在下面章节将会详细介绍两种任务及其应用场景。\n",
    "\n",
    "\n",
    "## Part B. 情感分析预训练模型SKEP\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的发展，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "情感预训练模型SKEP（Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis）。SKEP利用情感知识增强预训练模型， 在14项中英情感分析典型任务上全面超越SOTA，此工作已经被ACL 2020录用。SKEP是百度研究团队提出的基于情感知识增强的情感预训练算法，此算法采用无监督方法自动挖掘情感知识，然后利用情感知识构建预训练目标，从而让机器学会理解情感语义。SKEP为各类情感分析任务提供统一且强大的情感语义表示。\n",
    "\n",
    "**论文地址**：https://arxiv.org/abs/2005.05635\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep.png\" width=\"80%\" height=\"60%\"> <br />\n",
    "</p>\n",
    "\n",
    "百度研究团队在三个典型情感分析任务，句子级情感分类（Sentence-level Sentiment Classification），评价目标级情感分类（Aspect-level Sentiment Classification）、观点抽取（Opinion Role Labeling），共计14个中英文数据上进一步验证了情感预训练模型SKEP的效果。\n",
    "\n",
    "具体实验效果参考：https://github.com/baidu/Senta#skep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part C 句子级情感分析 & 目标级情感分析\n",
    "\n",
    "### Part C.1 句子级情感分析\n",
    "\n",
    "\n",
    "对给定的一段文本进行情感极性分类，常用于影评分析、网络论坛舆情分析等场景。如:\n",
    "\n",
    "```text\n",
    "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\t1\n",
    "15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错\t1\n",
    "房间太小。其他的都一般。。。。。。。。。\t0\n",
    "```\n",
    "\n",
    "其中`1`表示正向情感，`0`表示负向情感。\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4aae00a800ae4831b6811b669f7461d8482344b183454d8fb7d37c83defb9567\" width=\"550\" ></center>\n",
    "<br><center>句子级情感分析任务</center></br>\n",
    "\n",
    "\n",
    "#### 常用数据集\n",
    "\n",
    "ChnSenticorp数据集是公开中文情感分析常用数据集， 其为2分类数据集。PaddleNLP已经内置该数据集，一键即可加载。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\n",
      "--2021-06-18 10:26:25--  https://dataset-bj.cdn.bcebos.com/qianyan/NLPCC14-SC.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 182.61.128.166\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|182.61.128.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1262929 (1.2M) [application/zip]\n",
      "Saving to: ‘/home/aistudio/data/NLPCC14-SC.zip’\n",
      "\n",
      "/home/aistudio/data 100%[===================>]   1.20M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-06-18 10:26:25 (37.3 MB/s) - ‘/home/aistudio/data/NLPCC14-SC.zip’ saved [1262929/1262929]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\r\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/NLPCC14-SC.zip -O ~/data/NLPCC14-SC.zip\r\n",
    "!unzip -oq /home/aistudio/data/NLPCC14-SC.zip -d ~/data/\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 0, 'label': '1', 'text': '请问这机不是有个遥控器的吗？'}\n",
      "{'qid': 1, 'label': '1', 'text': '发短信特别不方便！背后的屏幕很大用起来不舒服，是手触屏的！切换屏幕很麻烦！'}\n",
      "{'qid': 2, 'label': '1', 'text': '手感超好，而且黑色相比白色在转得时候不容易眼花，找童年的记忆啦。'}\n",
      "{'qid': '0', 'label': '', 'text': '我终于找到同道中人啦～～～～从初中开始，我就已经喜欢上了michaeljackson.但同学们都用鄙夷的眼光看我，他们人为jackson的样子古怪甚至说＂丑＂．我当场气晕．但现在有同道中人了，我好开心！！！michaeljacksonisthemostsuccessfulsingerintheworld!!~~~'}\n",
      "{'qid': '1', 'label': '', 'text': '看完已是深夜两点，我却坐在电脑前情难自禁，这是最好的结局。惟有如此，就让那前世今生的纠结就停留在此刻。再相逢时，愿他的人生不再让人唏嘘，他们的身心也会只居一处。可是还是痛心为这样的人，这样的爱……'}\n",
      "{'qid': '2', 'label': '', 'text': '袁阔成先生是当今评书界的泰斗，十二金钱镖是他的代表作之一'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.datasets import MapDataset\n",
    "# train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# train_ds, dev_ds, test_ds = load_dataset(\"ChnSentiCorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            # next(fp)  # Skip header\n",
    "            cnt=0\n",
    "            title_list = fp.readline().strip('\\n').split('\\t')\n",
    "            for line in fp.readlines():\n",
    "                if 'qid' not in title_list:\n",
    "                    qid = cnt\n",
    "                    cnt = cnt+1\n",
    "                    labels, texts = line.strip('\\n').split('\\t')\n",
    "                elif 'label' not in title_list:\n",
    "                    labels=''\n",
    "                    qid, texts = line.strip('\\n').split('\\t')\n",
    "                else:\n",
    "                    qid, labels, texts = line.strip('\\n').split('\\t')\n",
    "                # words = words.split('\\002')\n",
    "                # labels = labels.split('\\002')\n",
    "                #  {'text': text, 'label': labels, 'qid':''}\n",
    "                yield {'qid': qid, 'label':labels, 'text':texts}\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds,  test_ds = load_dataset(datafiles=(\n",
    "        './data/NLPCC14-SC/train.tsv', './data/NLPCC14-SC/test.tsv'))\n",
    "# train_ds = load_dataset(datafiles=('./data/ChnSentiCorp/dev.tsv'))\n",
    "\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])\n",
    "print(train_ds[2])\n",
    "\n",
    "print(test_ds[0])\n",
    "print(test_ds[1])\n",
    "print(test_ds[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### SKEP模型加载\n",
    "\n",
    "PaddleNLP已经实现了SKEP预训练模型，可以通过一行代码实现SKEP加载。\n",
    "\n",
    "句子级情感分析模型是SKEP fine-tune 文本分类常用模型`SkepForSequenceClassification`。其首先通过SKEP提取句子语义特征，之后将语义特征进行分类。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc21e1201154451a80f32e0daa5fa84386c1b12e4b3244e387ae0b177c1dc963)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-18 12:06:39,390] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-18 12:06:49,213] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "\n",
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=2)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`SkepForSequenceClassification`可用于句子级情感分析和目标级情感分析任务。其通过预训练模型SKEP获取输入文本的表示，之后将文本表示进行分类。\n",
    "\n",
    "* `pretrained_model_name_or_path`：模型名称。支持\"skep_ernie_1.0_large_ch\"，\"skep_ernie_2.0_large_en\"。\n",
    "\t- \"skep_ernie_1.0_large_ch\"：是SKEP模型在预训练ernie_1.0_large_ch基础之上在海量中文数据上继续预训练得到的中文预训练模型；\n",
    "    - \"skep_ernie_2.0_large_en\"：是SKEP模型在预训练ernie_2.0_large_en基础之上在海量英文数据上继续预训练得到的英文预训练模型；\n",
    "    \n",
    "* `num_classes`: 数据集分类类别数。\n",
    "\n",
    "\n",
    "关于SKEP模型实现详细信息参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始ChnSentiCorp数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "from utils import create_dataloader\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=256,\n",
    "                    is_test=False):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`int`, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\n",
    "\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        # label：情感极性类别\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        # qid：每条数据的编号\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 24\n",
    "# 文本序列最大长度\n",
    "max_seq_length = 256\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "# dev_data_loader = create_dataloader(\n",
    "#     dev_ds,\n",
    "#     mode='dev',\n",
    "#     batch_size=batch_size,\n",
    "#     batchify_fn=batchify_fn,\n",
    "#     trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练和评估\n",
    "\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。\n",
    "\n",
    "\n",
    "**推荐超参设置：**\n",
    "\n",
    "* `max_seq_length=256`\n",
    "* `batch_size=48`\n",
    "* `learning_rate=2e-5`\n",
    "* `epochs=10`\n",
    "\n",
    "实际运行时可以根据显存大小调整batch_size和max_seq_length大小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils import evaluate\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 训练过程中保存模型参数的文件夹\n",
    "ckpt_dir = \"skep_ckpt_NLPCC14-SC\"\n",
    "# len(train_data_loader)一轮训练所需要的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=2e-5,\n",
    "    parameters=model.parameters())\n",
    "# 交叉熵损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.29067, accu: 0.85659, speed: 1.06 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.09907, accu: 0.85746, speed: 1.02 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.10869, accu: 0.85832, speed: 1.12 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.07327, accu: 0.85916, speed: 1.09 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.20912, accu: 0.86003, speed: 1.38 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.04829, accu: 0.86112, speed: 0.95 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.02235, accu: 0.86187, speed: 1.15 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.07145, accu: 0.86245, speed: 1.08 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.12644, accu: 0.86326, speed: 1.03 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.11074, accu: 0.86386, speed: 1.11 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.21706, accu: 0.86456, speed: 1.09 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.02368, accu: 0.86510, speed: 1.18 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.03642, accu: 0.86607, speed: 1.09 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.04015, accu: 0.86684, speed: 1.19 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.22934, accu: 0.86760, speed: 1.21 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.05031, accu: 0.86835, speed: 1.39 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.19564, accu: 0.86876, speed: 1.05 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.08623, accu: 0.86945, speed: 0.98 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.24397, accu: 0.87010, speed: 1.13 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.07319, accu: 0.87094, speed: 1.01 step/s\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.07254, accu: 0.87155, speed: 1.03 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.21633, accu: 0.87216, speed: 1.11 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.23406, accu: 0.87270, speed: 1.05 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.23761, accu: 0.87322, speed: 1.15 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.05550, accu: 0.87383, speed: 1.03 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.14279, accu: 0.87447, speed: 1.13 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.15558, accu: 0.87503, speed: 1.22 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.09986, accu: 0.87529, speed: 1.14 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.04915, accu: 0.87581, speed: 1.18 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.06950, accu: 0.87619, speed: 1.27 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.16911, accu: 0.87669, speed: 1.01 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.06966, accu: 0.87734, speed: 1.06 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.24417, accu: 0.87782, speed: 0.98 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.41454, accu: 0.87820, speed: 1.09 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.36406, accu: 0.87867, speed: 1.02 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.19001, accu: 0.87908, speed: 1.02 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.09273, accu: 0.87953, speed: 1.19 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.18949, accu: 0.87980, speed: 0.97 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.08311, accu: 0.88025, speed: 1.13 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.21648, accu: 0.88063, speed: 1.19 step/s\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.28346, accu: 0.88106, speed: 1.16 step/s\n",
      "global step 420, epoch: 2, batch: 3, loss: 0.00682, accu: 0.88155, speed: 1.29 step/s\n",
      "global step 430, epoch: 2, batch: 13, loss: 0.11134, accu: 0.88220, speed: 1.08 step/s\n",
      "global step 440, epoch: 2, batch: 23, loss: 0.06843, accu: 0.88285, speed: 1.22 step/s\n",
      "global step 450, epoch: 2, batch: 33, loss: 0.01871, accu: 0.88357, speed: 1.17 step/s\n",
      "global step 460, epoch: 2, batch: 43, loss: 0.01250, accu: 0.88434, speed: 1.07 step/s\n",
      "global step 470, epoch: 2, batch: 53, loss: 0.22159, accu: 0.88507, speed: 1.08 step/s\n",
      "global step 480, epoch: 2, batch: 63, loss: 0.00487, accu: 0.88576, speed: 1.11 step/s\n",
      "global step 490, epoch: 2, batch: 73, loss: 0.04200, accu: 0.88636, speed: 1.14 step/s\n",
      "global step 500, epoch: 2, batch: 83, loss: 0.00896, accu: 0.88701, speed: 1.02 step/s\n",
      "global step 510, epoch: 2, batch: 93, loss: 0.02197, accu: 0.88756, speed: 0.97 step/s\n",
      "global step 520, epoch: 2, batch: 103, loss: 0.05470, accu: 0.88817, speed: 1.12 step/s\n",
      "global step 530, epoch: 2, batch: 113, loss: 0.02018, accu: 0.88884, speed: 1.02 step/s\n",
      "global step 540, epoch: 2, batch: 123, loss: 0.00328, accu: 0.88951, speed: 0.99 step/s\n",
      "global step 550, epoch: 2, batch: 133, loss: 0.16062, accu: 0.89012, speed: 1.17 step/s\n",
      "global step 560, epoch: 2, batch: 143, loss: 0.11820, accu: 0.89045, speed: 1.11 step/s\n",
      "global step 570, epoch: 2, batch: 153, loss: 0.11901, accu: 0.89104, speed: 1.23 step/s\n",
      "global step 580, epoch: 2, batch: 163, loss: 0.16199, accu: 0.89160, speed: 1.01 step/s\n",
      "global step 590, epoch: 2, batch: 173, loss: 0.01515, accu: 0.89223, speed: 1.15 step/s\n",
      "global step 600, epoch: 2, batch: 183, loss: 0.03658, accu: 0.89282, speed: 1.15 step/s\n",
      "global step 610, epoch: 2, batch: 193, loss: 0.01384, accu: 0.89339, speed: 1.22 step/s\n",
      "global step 620, epoch: 2, batch: 203, loss: 0.21190, accu: 0.89397, speed: 1.10 step/s\n",
      "global step 630, epoch: 2, batch: 213, loss: 0.01795, accu: 0.89446, speed: 1.10 step/s\n",
      "global step 640, epoch: 2, batch: 223, loss: 0.00877, accu: 0.89501, speed: 1.08 step/s\n",
      "global step 650, epoch: 2, batch: 233, loss: 0.03724, accu: 0.89557, speed: 1.04 step/s\n",
      "global step 660, epoch: 2, batch: 243, loss: 0.13523, accu: 0.89615, speed: 1.17 step/s\n",
      "global step 670, epoch: 2, batch: 253, loss: 0.07338, accu: 0.89654, speed: 1.14 step/s\n",
      "global step 680, epoch: 2, batch: 263, loss: 0.02771, accu: 0.89706, speed: 1.06 step/s\n",
      "global step 690, epoch: 2, batch: 273, loss: 0.16563, accu: 0.89740, speed: 1.08 step/s\n",
      "global step 700, epoch: 2, batch: 283, loss: 0.01303, accu: 0.89781, speed: 1.11 step/s\n",
      "global step 710, epoch: 2, batch: 293, loss: 0.04428, accu: 0.89826, speed: 1.19 step/s\n",
      "global step 720, epoch: 2, batch: 303, loss: 0.18723, accu: 0.89861, speed: 0.97 step/s\n",
      "global step 730, epoch: 2, batch: 313, loss: 0.01593, accu: 0.89903, speed: 1.05 step/s\n",
      "global step 740, epoch: 2, batch: 323, loss: 0.01729, accu: 0.89944, speed: 1.04 step/s\n",
      "global step 750, epoch: 2, batch: 333, loss: 0.09713, accu: 0.89968, speed: 1.06 step/s\n",
      "global step 760, epoch: 2, batch: 343, loss: 0.15076, accu: 0.90006, speed: 1.00 step/s\n",
      "global step 770, epoch: 2, batch: 353, loss: 0.02049, accu: 0.90051, speed: 1.04 step/s\n",
      "global step 780, epoch: 2, batch: 363, loss: 0.06167, accu: 0.90097, speed: 1.09 step/s\n",
      "global step 790, epoch: 2, batch: 373, loss: 0.18433, accu: 0.90143, speed: 1.07 step/s\n",
      "global step 800, epoch: 2, batch: 383, loss: 0.15975, accu: 0.90173, speed: 0.98 step/s\n",
      "global step 810, epoch: 2, batch: 393, loss: 0.04604, accu: 0.90216, speed: 1.28 step/s\n",
      "global step 820, epoch: 2, batch: 403, loss: 0.24892, accu: 0.90249, speed: 1.17 step/s\n",
      "global step 830, epoch: 2, batch: 413, loss: 0.06967, accu: 0.90291, speed: 1.14 step/s\n",
      "global step 840, epoch: 3, batch: 6, loss: 0.04345, accu: 0.90330, speed: 1.13 step/s\n",
      "global step 850, epoch: 3, batch: 16, loss: 0.00590, accu: 0.90380, speed: 1.08 step/s\n",
      "global step 860, epoch: 3, batch: 26, loss: 0.00546, accu: 0.90428, speed: 1.11 step/s\n",
      "global step 870, epoch: 3, batch: 36, loss: 0.16631, accu: 0.90472, speed: 1.15 step/s\n",
      "global step 880, epoch: 3, batch: 46, loss: 0.01711, accu: 0.90519, speed: 1.18 step/s\n",
      "global step 890, epoch: 3, batch: 56, loss: 0.19221, accu: 0.90560, speed: 1.12 step/s\n",
      "global step 900, epoch: 3, batch: 66, loss: 0.00739, accu: 0.90599, speed: 1.05 step/s\n",
      "global step 910, epoch: 3, batch: 76, loss: 0.00610, accu: 0.90646, speed: 1.15 step/s\n",
      "global step 920, epoch: 3, batch: 86, loss: 0.12681, accu: 0.90686, speed: 1.14 step/s\n",
      "global step 930, epoch: 3, batch: 96, loss: 0.03895, accu: 0.90723, speed: 1.07 step/s\n",
      "global step 940, epoch: 3, batch: 106, loss: 0.00326, accu: 0.90769, speed: 1.01 step/s\n",
      "global step 950, epoch: 3, batch: 116, loss: 0.02283, accu: 0.90812, speed: 0.98 step/s\n",
      "global step 960, epoch: 3, batch: 126, loss: 0.10083, accu: 0.90848, speed: 1.03 step/s\n",
      "global step 970, epoch: 3, batch: 136, loss: 0.00244, accu: 0.90893, speed: 1.12 step/s\n",
      "global step 980, epoch: 3, batch: 146, loss: 0.01857, accu: 0.90928, speed: 1.25 step/s\n",
      "global step 990, epoch: 3, batch: 156, loss: 0.00616, accu: 0.90968, speed: 1.01 step/s\n",
      "global step 1000, epoch: 3, batch: 166, loss: 0.05813, accu: 0.91004, speed: 1.14 step/s\n",
      "global step 1010, epoch: 3, batch: 176, loss: 0.01770, accu: 0.91041, speed: 0.39 step/s\n",
      "global step 1020, epoch: 3, batch: 186, loss: 0.02875, accu: 0.91081, speed: 1.08 step/s\n",
      "global step 1030, epoch: 3, batch: 196, loss: 0.01598, accu: 0.91117, speed: 0.98 step/s\n",
      "global step 1040, epoch: 3, batch: 206, loss: 0.01483, accu: 0.91156, speed: 1.02 step/s\n",
      "global step 1050, epoch: 3, batch: 216, loss: 0.03760, accu: 0.91189, speed: 1.02 step/s\n",
      "global step 1060, epoch: 3, batch: 226, loss: 0.08843, accu: 0.91216, speed: 1.18 step/s\n",
      "global step 1070, epoch: 3, batch: 236, loss: 0.06468, accu: 0.91246, speed: 1.06 step/s\n",
      "global step 1080, epoch: 3, batch: 246, loss: 0.06422, accu: 0.91280, speed: 1.01 step/s\n",
      "global step 1090, epoch: 3, batch: 256, loss: 0.03804, accu: 0.91320, speed: 1.05 step/s\n",
      "global step 1100, epoch: 3, batch: 266, loss: 0.00741, accu: 0.91358, speed: 1.02 step/s\n",
      "global step 1110, epoch: 3, batch: 276, loss: 0.00238, accu: 0.91389, speed: 1.06 step/s\n",
      "global step 1120, epoch: 3, batch: 286, loss: 0.09165, accu: 0.91423, speed: 1.25 step/s\n",
      "global step 1130, epoch: 3, batch: 296, loss: 0.07565, accu: 0.91452, speed: 1.07 step/s\n",
      "global step 1140, epoch: 3, batch: 306, loss: 0.11327, accu: 0.91480, speed: 1.06 step/s\n",
      "global step 1150, epoch: 3, batch: 316, loss: 0.02649, accu: 0.91510, speed: 1.08 step/s\n",
      "global step 1160, epoch: 3, batch: 326, loss: 0.08242, accu: 0.91542, speed: 0.99 step/s\n",
      "global step 1170, epoch: 3, batch: 336, loss: 0.14061, accu: 0.91560, speed: 1.11 step/s\n",
      "global step 1180, epoch: 3, batch: 346, loss: 0.05248, accu: 0.91589, speed: 1.11 step/s\n",
      "global step 1190, epoch: 3, batch: 356, loss: 0.01320, accu: 0.91616, speed: 1.06 step/s\n",
      "global step 1200, epoch: 3, batch: 366, loss: 0.04485, accu: 0.91648, speed: 1.01 step/s\n",
      "global step 1210, epoch: 3, batch: 376, loss: 0.04062, accu: 0.91673, speed: 1.18 step/s\n",
      "global step 1220, epoch: 3, batch: 386, loss: 0.06019, accu: 0.91707, speed: 1.17 step/s\n",
      "global step 1230, epoch: 3, batch: 396, loss: 0.01404, accu: 0.91735, speed: 1.14 step/s\n",
      "global step 1240, epoch: 3, batch: 406, loss: 0.03699, accu: 0.91766, speed: 1.05 step/s\n",
      "global step 1250, epoch: 3, batch: 416, loss: 0.08996, accu: 0.91793, speed: 1.04 step/s\n",
      "global step 1260, epoch: 4, batch: 9, loss: 0.00982, accu: 0.91827, speed: 1.19 step/s\n",
      "global step 1270, epoch: 4, batch: 19, loss: 0.01365, accu: 0.91857, speed: 1.00 step/s\n",
      "global step 1280, epoch: 4, batch: 29, loss: 0.00204, accu: 0.91887, speed: 1.18 step/s\n",
      "global step 1290, epoch: 4, batch: 39, loss: 0.00260, accu: 0.91916, speed: 1.07 step/s\n",
      "global step 1300, epoch: 4, batch: 49, loss: 0.00754, accu: 0.91947, speed: 1.20 step/s\n",
      "global step 1310, epoch: 4, batch: 59, loss: 0.00695, accu: 0.91980, speed: 1.17 step/s\n",
      "global step 1320, epoch: 4, batch: 69, loss: 0.10714, accu: 0.92011, speed: 1.03 step/s\n",
      "global step 1330, epoch: 4, batch: 79, loss: 0.00828, accu: 0.92044, speed: 1.24 step/s\n",
      "global step 1340, epoch: 4, batch: 89, loss: 0.00764, accu: 0.92076, speed: 0.98 step/s\n",
      "global step 1350, epoch: 4, batch: 99, loss: 0.02935, accu: 0.92110, speed: 0.99 step/s\n",
      "global step 1360, epoch: 4, batch: 109, loss: 0.01092, accu: 0.92136, speed: 1.15 step/s\n",
      "global step 1370, epoch: 4, batch: 119, loss: 0.00118, accu: 0.92169, speed: 1.18 step/s\n",
      "global step 1380, epoch: 4, batch: 129, loss: 0.00180, accu: 0.92200, speed: 1.13 step/s\n",
      "global step 1390, epoch: 4, batch: 139, loss: 0.03192, accu: 0.92231, speed: 1.06 step/s\n",
      "global step 1400, epoch: 4, batch: 149, loss: 0.03271, accu: 0.92262, speed: 1.13 step/s\n",
      "global step 1410, epoch: 4, batch: 159, loss: 0.09649, accu: 0.92291, speed: 1.00 step/s\n",
      "global step 1420, epoch: 4, batch: 169, loss: 0.00361, accu: 0.92317, speed: 1.36 step/s\n",
      "global step 1430, epoch: 4, batch: 179, loss: 0.00172, accu: 0.92342, speed: 1.00 step/s\n",
      "global step 1440, epoch: 4, batch: 189, loss: 0.01571, accu: 0.92367, speed: 1.13 step/s\n",
      "global step 1450, epoch: 4, batch: 199, loss: 0.12094, accu: 0.92394, speed: 1.10 step/s\n",
      "global step 1460, epoch: 4, batch: 209, loss: 0.00949, accu: 0.92425, speed: 1.04 step/s\n",
      "global step 1470, epoch: 4, batch: 219, loss: 0.02388, accu: 0.92454, speed: 1.07 step/s\n",
      "global step 1480, epoch: 4, batch: 229, loss: 0.00438, accu: 0.92482, speed: 1.03 step/s\n",
      "global step 1490, epoch: 4, batch: 239, loss: 0.00081, accu: 0.92507, speed: 1.08 step/s\n",
      "global step 1500, epoch: 4, batch: 249, loss: 0.00151, accu: 0.92535, speed: 1.02 step/s\n",
      "global step 1510, epoch: 4, batch: 259, loss: 0.00159, accu: 0.92561, speed: 1.00 step/s\n",
      "global step 1520, epoch: 4, batch: 269, loss: 0.01163, accu: 0.92583, speed: 1.10 step/s\n",
      "global step 1530, epoch: 4, batch: 279, loss: 0.01073, accu: 0.92610, speed: 1.14 step/s\n",
      "global step 1540, epoch: 4, batch: 289, loss: 0.03521, accu: 0.92635, speed: 1.09 step/s\n",
      "global step 1550, epoch: 4, batch: 299, loss: 0.18065, accu: 0.92654, speed: 1.02 step/s\n",
      "global step 1560, epoch: 4, batch: 309, loss: 0.00279, accu: 0.92679, speed: 1.05 step/s\n",
      "global step 1570, epoch: 4, batch: 319, loss: 0.00512, accu: 0.92708, speed: 1.18 step/s\n",
      "global step 1580, epoch: 4, batch: 329, loss: 0.13314, accu: 0.92731, speed: 1.10 step/s\n",
      "global step 1590, epoch: 4, batch: 339, loss: 0.04987, accu: 0.92751, speed: 1.16 step/s\n",
      "global step 1600, epoch: 4, batch: 349, loss: 0.04516, accu: 0.92774, speed: 1.08 step/s\n",
      "global step 1610, epoch: 4, batch: 359, loss: 0.00231, accu: 0.92799, speed: 1.10 step/s\n",
      "global step 1620, epoch: 4, batch: 369, loss: 0.00596, accu: 0.92821, speed: 1.12 step/s\n",
      "global step 1630, epoch: 4, batch: 379, loss: 0.00640, accu: 0.92846, speed: 0.98 step/s\n",
      "global step 1640, epoch: 4, batch: 389, loss: 0.03609, accu: 0.92863, speed: 1.20 step/s\n",
      "global step 1650, epoch: 4, batch: 399, loss: 0.02966, accu: 0.92881, speed: 1.00 step/s\n",
      "global step 1660, epoch: 4, batch: 409, loss: 0.01171, accu: 0.92903, speed: 0.94 step/s\n",
      "global step 1670, epoch: 5, batch: 2, loss: 0.00483, accu: 0.92927, speed: 1.16 step/s\n",
      "global step 1680, epoch: 5, batch: 12, loss: 0.01272, accu: 0.92950, speed: 1.05 step/s\n",
      "global step 1690, epoch: 5, batch: 22, loss: 0.06640, accu: 0.92972, speed: 1.02 step/s\n",
      "global step 1700, epoch: 5, batch: 32, loss: 0.00538, accu: 0.92995, speed: 1.17 step/s\n",
      "global step 1710, epoch: 5, batch: 42, loss: 0.00567, accu: 0.93015, speed: 1.27 step/s\n",
      "global step 1720, epoch: 5, batch: 52, loss: 0.00204, accu: 0.93039, speed: 1.11 step/s\n",
      "global step 1730, epoch: 5, batch: 62, loss: 0.00022, accu: 0.93058, speed: 0.98 step/s\n",
      "global step 1740, epoch: 5, batch: 72, loss: 0.00150, accu: 0.93077, speed: 1.23 step/s\n",
      "global step 1750, epoch: 5, batch: 82, loss: 0.17878, accu: 0.93096, speed: 1.18 step/s\n",
      "global step 1760, epoch: 5, batch: 92, loss: 0.00437, accu: 0.93119, speed: 0.98 step/s\n",
      "global step 1770, epoch: 5, batch: 102, loss: 0.04672, accu: 0.93137, speed: 1.04 step/s\n",
      "global step 1780, epoch: 5, batch: 112, loss: 0.01237, accu: 0.93159, speed: 1.07 step/s\n",
      "global step 1790, epoch: 5, batch: 122, loss: 0.00285, accu: 0.93179, speed: 1.06 step/s\n",
      "global step 1800, epoch: 5, batch: 132, loss: 0.00506, accu: 0.93199, speed: 1.02 step/s\n",
      "global step 1810, epoch: 5, batch: 142, loss: 0.02047, accu: 0.93217, speed: 1.22 step/s\n",
      "global step 1820, epoch: 5, batch: 152, loss: 0.24539, accu: 0.93232, speed: 1.10 step/s\n",
      "global step 1830, epoch: 5, batch: 162, loss: 0.00973, accu: 0.93251, speed: 1.07 step/s\n",
      "global step 1840, epoch: 5, batch: 172, loss: 0.00203, accu: 0.93271, speed: 1.04 step/s\n",
      "global step 1850, epoch: 5, batch: 182, loss: 0.09322, accu: 0.93286, speed: 1.10 step/s\n",
      "global step 1860, epoch: 5, batch: 192, loss: 0.00398, accu: 0.93305, speed: 1.18 step/s\n",
      "global step 1870, epoch: 5, batch: 202, loss: 0.03756, accu: 0.93327, speed: 1.07 step/s\n",
      "global step 1880, epoch: 5, batch: 212, loss: 0.00489, accu: 0.93348, speed: 1.12 step/s\n",
      "global step 1890, epoch: 5, batch: 222, loss: 0.00643, accu: 0.93369, speed: 1.22 step/s\n",
      "global step 1900, epoch: 5, batch: 232, loss: 0.00769, accu: 0.93391, speed: 1.16 step/s\n",
      "global step 1910, epoch: 5, batch: 242, loss: 0.01505, accu: 0.93412, speed: 1.03 step/s\n",
      "global step 1920, epoch: 5, batch: 252, loss: 0.00085, accu: 0.93432, speed: 0.97 step/s\n",
      "global step 1930, epoch: 5, batch: 262, loss: 0.00058, accu: 0.93451, speed: 1.03 step/s\n",
      "global step 1940, epoch: 5, batch: 272, loss: 0.00382, accu: 0.93468, speed: 1.07 step/s\n",
      "global step 1950, epoch: 5, batch: 282, loss: 0.06764, accu: 0.93481, speed: 1.08 step/s\n",
      "global step 1960, epoch: 5, batch: 292, loss: 0.02460, accu: 0.93499, speed: 1.01 step/s\n",
      "global step 1970, epoch: 5, batch: 302, loss: 0.19573, accu: 0.93517, speed: 1.14 step/s\n",
      "global step 1980, epoch: 5, batch: 312, loss: 0.02816, accu: 0.93535, speed: 1.04 step/s\n",
      "global step 1990, epoch: 5, batch: 322, loss: 0.01998, accu: 0.93552, speed: 1.08 step/s\n",
      "global step 2000, epoch: 5, batch: 332, loss: 0.00036, accu: 0.93569, speed: 1.18 step/s\n",
      "global step 2010, epoch: 5, batch: 342, loss: 0.14802, accu: 0.93584, speed: 0.41 step/s\n",
      "global step 2020, epoch: 5, batch: 352, loss: 0.01658, accu: 0.93602, speed: 1.11 step/s\n",
      "global step 2030, epoch: 5, batch: 362, loss: 0.04995, accu: 0.93619, speed: 1.14 step/s\n",
      "global step 2040, epoch: 5, batch: 372, loss: 0.05733, accu: 0.93633, speed: 1.08 step/s\n",
      "global step 2050, epoch: 5, batch: 382, loss: 0.01625, accu: 0.93646, speed: 1.14 step/s\n",
      "global step 2060, epoch: 5, batch: 392, loss: 0.00365, accu: 0.93666, speed: 1.01 step/s\n",
      "global step 2070, epoch: 5, batch: 402, loss: 0.03326, accu: 0.93685, speed: 1.08 step/s\n",
      "global step 2080, epoch: 5, batch: 412, loss: 0.00649, accu: 0.93704, speed: 1.17 step/s\n",
      "global step 2090, epoch: 6, batch: 5, loss: 0.47597, accu: 0.93718, speed: 1.08 step/s\n",
      "global step 2100, epoch: 6, batch: 15, loss: 0.00224, accu: 0.93737, speed: 1.07 step/s\n",
      "global step 2110, epoch: 6, batch: 25, loss: 0.00470, accu: 0.93755, speed: 1.05 step/s\n",
      "global step 2120, epoch: 6, batch: 35, loss: 0.00909, accu: 0.93775, speed: 1.14 step/s\n",
      "global step 2130, epoch: 6, batch: 45, loss: 0.00127, accu: 0.93792, speed: 1.19 step/s\n",
      "global step 2140, epoch: 6, batch: 55, loss: 0.00074, accu: 0.93812, speed: 1.08 step/s\n",
      "global step 2150, epoch: 6, batch: 65, loss: 0.00294, accu: 0.93831, speed: 1.27 step/s\n",
      "global step 2160, epoch: 6, batch: 75, loss: 0.01795, accu: 0.93851, speed: 1.14 step/s\n",
      "global step 2170, epoch: 6, batch: 85, loss: 0.08985, accu: 0.93868, speed: 1.10 step/s\n",
      "global step 2180, epoch: 6, batch: 95, loss: 0.00097, accu: 0.93887, speed: 1.13 step/s\n",
      "global step 2190, epoch: 6, batch: 105, loss: 0.00486, accu: 0.93905, speed: 1.21 step/s\n",
      "global step 2200, epoch: 6, batch: 115, loss: 0.00099, accu: 0.93924, speed: 1.02 step/s\n",
      "global step 2210, epoch: 6, batch: 125, loss: 0.00034, accu: 0.93940, speed: 1.06 step/s\n",
      "global step 2220, epoch: 6, batch: 135, loss: 0.00156, accu: 0.93959, speed: 1.01 step/s\n",
      "global step 2230, epoch: 6, batch: 145, loss: 0.00154, accu: 0.93977, speed: 0.98 step/s\n",
      "global step 2240, epoch: 6, batch: 155, loss: 0.01122, accu: 0.93995, speed: 1.14 step/s\n",
      "global step 2250, epoch: 6, batch: 165, loss: 0.30009, accu: 0.94007, speed: 1.12 step/s\n",
      "global step 2260, epoch: 6, batch: 175, loss: 0.01128, accu: 0.94024, speed: 1.12 step/s\n",
      "global step 2270, epoch: 6, batch: 185, loss: 0.00117, accu: 0.94041, speed: 1.16 step/s\n",
      "global step 2280, epoch: 6, batch: 195, loss: 0.01540, accu: 0.94056, speed: 1.04 step/s\n",
      "global step 2290, epoch: 6, batch: 205, loss: 0.04768, accu: 0.94069, speed: 1.05 step/s\n",
      "global step 2300, epoch: 6, batch: 215, loss: 0.00218, accu: 0.94085, speed: 1.13 step/s\n",
      "global step 2310, epoch: 6, batch: 225, loss: 0.00379, accu: 0.94100, speed: 1.04 step/s\n",
      "global step 2320, epoch: 6, batch: 235, loss: 0.00949, accu: 0.94115, speed: 0.99 step/s\n",
      "global step 2330, epoch: 6, batch: 245, loss: 0.04639, accu: 0.94126, speed: 1.09 step/s\n",
      "global step 2340, epoch: 6, batch: 255, loss: 0.10978, accu: 0.94140, speed: 1.10 step/s\n",
      "global step 2350, epoch: 6, batch: 265, loss: 0.00230, accu: 0.94156, speed: 0.98 step/s\n",
      "global step 2360, epoch: 6, batch: 275, loss: 0.00243, accu: 0.94169, speed: 1.11 step/s\n",
      "global step 2370, epoch: 6, batch: 285, loss: 0.06704, accu: 0.94180, speed: 1.06 step/s\n",
      "global step 2380, epoch: 6, batch: 295, loss: 0.13061, accu: 0.94191, speed: 1.07 step/s\n",
      "global step 2390, epoch: 6, batch: 305, loss: 0.00347, accu: 0.94207, speed: 1.14 step/s\n",
      "global step 2400, epoch: 6, batch: 315, loss: 0.00428, accu: 0.94223, speed: 1.12 step/s\n",
      "global step 2410, epoch: 6, batch: 325, loss: 0.01399, accu: 0.94237, speed: 1.07 step/s\n",
      "global step 2420, epoch: 6, batch: 335, loss: 0.00223, accu: 0.94250, speed: 1.05 step/s\n",
      "global step 2430, epoch: 6, batch: 345, loss: 0.01052, accu: 0.94265, speed: 1.18 step/s\n",
      "global step 2440, epoch: 6, batch: 355, loss: 0.00477, accu: 0.94277, speed: 1.09 step/s\n",
      "global step 2450, epoch: 6, batch: 365, loss: 0.05597, accu: 0.94286, speed: 1.19 step/s\n",
      "global step 2460, epoch: 6, batch: 375, loss: 0.00063, accu: 0.94299, speed: 1.08 step/s\n",
      "global step 2470, epoch: 6, batch: 385, loss: 0.00518, accu: 0.94312, speed: 1.11 step/s\n",
      "global step 2480, epoch: 6, batch: 395, loss: 0.00592, accu: 0.94327, speed: 0.99 step/s\n",
      "global step 2490, epoch: 6, batch: 405, loss: 0.00837, accu: 0.94340, speed: 1.05 step/s\n",
      "global step 2500, epoch: 6, batch: 415, loss: 0.00522, accu: 0.94353, speed: 1.04 step/s\n",
      "global step 2510, epoch: 7, batch: 8, loss: 0.01039, accu: 0.94369, speed: 1.15 step/s\n",
      "global step 2520, epoch: 7, batch: 18, loss: 0.00097, accu: 0.94385, speed: 0.96 step/s\n",
      "global step 2530, epoch: 7, batch: 28, loss: 0.00037, accu: 0.94400, speed: 1.02 step/s\n",
      "global step 2540, epoch: 7, batch: 38, loss: 0.00059, accu: 0.94416, speed: 1.04 step/s\n",
      "global step 2550, epoch: 7, batch: 48, loss: 0.00088, accu: 0.94430, speed: 0.99 step/s\n",
      "global step 2560, epoch: 7, batch: 58, loss: 0.03342, accu: 0.94442, speed: 1.00 step/s\n",
      "global step 2570, epoch: 7, batch: 68, loss: 0.00118, accu: 0.94458, speed: 1.07 step/s\n",
      "global step 2580, epoch: 7, batch: 78, loss: 0.20261, accu: 0.94472, speed: 1.30 step/s\n",
      "global step 2590, epoch: 7, batch: 88, loss: 0.00088, accu: 0.94488, speed: 1.15 step/s\n",
      "global step 2600, epoch: 7, batch: 98, loss: 0.13186, accu: 0.94498, speed: 0.98 step/s\n",
      "global step 2610, epoch: 7, batch: 108, loss: 0.00263, accu: 0.94509, speed: 1.25 step/s\n",
      "global step 2620, epoch: 7, batch: 118, loss: 0.00511, accu: 0.94523, speed: 1.05 step/s\n",
      "global step 2630, epoch: 7, batch: 128, loss: 0.02078, accu: 0.94538, speed: 1.19 step/s\n",
      "global step 2640, epoch: 7, batch: 138, loss: 0.00273, accu: 0.94552, speed: 1.01 step/s\n",
      "global step 2650, epoch: 7, batch: 148, loss: 0.00808, accu: 0.94566, speed: 1.04 step/s\n",
      "global step 2660, epoch: 7, batch: 158, loss: 0.00051, accu: 0.94581, speed: 1.09 step/s\n",
      "global step 2670, epoch: 7, batch: 168, loss: 0.09189, accu: 0.94593, speed: 1.17 step/s\n",
      "global step 2680, epoch: 7, batch: 178, loss: 0.00026, accu: 0.94608, speed: 1.01 step/s\n",
      "global step 2690, epoch: 7, batch: 188, loss: 0.00094, accu: 0.94621, speed: 1.07 step/s\n",
      "global step 2700, epoch: 7, batch: 198, loss: 0.00205, accu: 0.94635, speed: 1.24 step/s\n",
      "global step 2710, epoch: 7, batch: 208, loss: 0.00848, accu: 0.94648, speed: 1.09 step/s\n",
      "global step 2720, epoch: 7, batch: 218, loss: 0.00067, accu: 0.94661, speed: 1.01 step/s\n",
      "global step 2730, epoch: 7, batch: 228, loss: 0.00912, accu: 0.94674, speed: 1.06 step/s\n",
      "global step 2740, epoch: 7, batch: 238, loss: 0.00082, accu: 0.94688, speed: 1.18 step/s\n",
      "global step 2750, epoch: 7, batch: 248, loss: 0.00800, accu: 0.94701, speed: 1.11 step/s\n",
      "global step 2760, epoch: 7, batch: 258, loss: 0.00445, accu: 0.94714, speed: 1.10 step/s\n",
      "global step 2770, epoch: 7, batch: 268, loss: 0.00132, accu: 0.94728, speed: 1.07 step/s\n",
      "global step 2780, epoch: 7, batch: 278, loss: 0.00044, accu: 0.94741, speed: 1.02 step/s\n",
      "global step 2790, epoch: 7, batch: 288, loss: 0.00159, accu: 0.94754, speed: 1.07 step/s\n",
      "global step 2800, epoch: 7, batch: 298, loss: 0.00075, accu: 0.94767, speed: 1.13 step/s\n",
      "global step 2810, epoch: 7, batch: 308, loss: 0.14150, accu: 0.94779, speed: 1.01 step/s\n",
      "global step 2820, epoch: 7, batch: 318, loss: 0.00784, accu: 0.94791, speed: 1.11 step/s\n",
      "global step 2830, epoch: 7, batch: 328, loss: 0.00044, accu: 0.94804, speed: 1.16 step/s\n",
      "global step 2840, epoch: 7, batch: 338, loss: 0.13111, accu: 0.94816, speed: 1.05 step/s\n",
      "global step 2850, epoch: 7, batch: 348, loss: 0.02013, accu: 0.94829, speed: 1.24 step/s\n",
      "global step 2860, epoch: 7, batch: 358, loss: 0.00454, accu: 0.94841, speed: 1.08 step/s\n",
      "global step 2870, epoch: 7, batch: 368, loss: 0.00067, accu: 0.94854, speed: 1.06 step/s\n",
      "global step 2880, epoch: 7, batch: 378, loss: 0.00046, accu: 0.94865, speed: 1.02 step/s\n",
      "global step 2890, epoch: 7, batch: 388, loss: 0.00288, accu: 0.94878, speed: 1.07 step/s\n",
      "global step 2900, epoch: 7, batch: 398, loss: 0.00145, accu: 0.94891, speed: 1.13 step/s\n",
      "global step 2910, epoch: 7, batch: 408, loss: 0.02448, accu: 0.94903, speed: 1.01 step/s\n",
      "global step 2920, epoch: 8, batch: 1, loss: 0.00986, accu: 0.94911, speed: 1.15 step/s\n",
      "global step 2930, epoch: 8, batch: 11, loss: 0.00266, accu: 0.94924, speed: 1.11 step/s\n",
      "global step 2940, epoch: 8, batch: 21, loss: 0.00006, accu: 0.94936, speed: 1.18 step/s\n",
      "global step 2950, epoch: 8, batch: 31, loss: 0.00155, accu: 0.94948, speed: 1.07 step/s\n",
      "global step 2960, epoch: 8, batch: 41, loss: 0.00051, accu: 0.94960, speed: 1.09 step/s\n",
      "global step 2970, epoch: 8, batch: 51, loss: 0.00453, accu: 0.94972, speed: 1.14 step/s\n",
      "global step 2980, epoch: 8, batch: 61, loss: 0.03367, accu: 0.94985, speed: 1.17 step/s\n",
      "global step 2990, epoch: 8, batch: 71, loss: 0.02378, accu: 0.94997, speed: 1.17 step/s\n",
      "global step 3000, epoch: 8, batch: 81, loss: 0.00082, accu: 0.95009, speed: 1.12 step/s\n",
      "global step 3010, epoch: 8, batch: 91, loss: 0.00017, accu: 0.95021, speed: 0.41 step/s\n",
      "global step 3020, epoch: 8, batch: 101, loss: 0.04262, accu: 0.95032, speed: 0.97 step/s\n",
      "global step 3030, epoch: 8, batch: 111, loss: 0.00048, accu: 0.95042, speed: 1.09 step/s\n",
      "global step 3040, epoch: 8, batch: 121, loss: 0.05512, accu: 0.95051, speed: 1.15 step/s\n",
      "global step 3050, epoch: 8, batch: 131, loss: 0.25519, accu: 0.95060, speed: 1.17 step/s\n",
      "global step 3060, epoch: 8, batch: 141, loss: 0.00589, accu: 0.95068, speed: 1.22 step/s\n",
      "global step 3070, epoch: 8, batch: 151, loss: 0.02173, accu: 0.95077, speed: 1.19 step/s\n",
      "global step 3080, epoch: 8, batch: 161, loss: 0.01733, accu: 0.95088, speed: 1.13 step/s\n",
      "global step 3090, epoch: 8, batch: 171, loss: 0.00896, accu: 0.95095, speed: 0.98 step/s\n",
      "global step 3100, epoch: 8, batch: 181, loss: 0.00493, accu: 0.95106, speed: 1.06 step/s\n",
      "global step 3110, epoch: 8, batch: 191, loss: 0.00112, accu: 0.95118, speed: 1.11 step/s\n",
      "global step 3120, epoch: 8, batch: 201, loss: 0.00040, accu: 0.95130, speed: 1.04 step/s\n",
      "global step 3130, epoch: 8, batch: 211, loss: 0.00644, accu: 0.95140, speed: 1.03 step/s\n",
      "global step 3140, epoch: 8, batch: 221, loss: 0.00281, accu: 0.95150, speed: 1.31 step/s\n",
      "global step 3150, epoch: 8, batch: 231, loss: 0.01426, accu: 0.95161, speed: 1.15 step/s\n",
      "global step 3160, epoch: 8, batch: 241, loss: 0.00023, accu: 0.95171, speed: 1.02 step/s\n",
      "global step 3170, epoch: 8, batch: 251, loss: 0.00232, accu: 0.95181, speed: 1.04 step/s\n",
      "global step 3180, epoch: 8, batch: 261, loss: 0.00295, accu: 0.95189, speed: 1.14 step/s\n",
      "global step 3190, epoch: 8, batch: 271, loss: 0.00547, accu: 0.95195, speed: 1.01 step/s\n",
      "global step 3200, epoch: 8, batch: 281, loss: 0.00605, accu: 0.95207, speed: 1.15 step/s\n",
      "global step 3210, epoch: 8, batch: 291, loss: 0.00040, accu: 0.95218, speed: 1.09 step/s\n",
      "global step 3220, epoch: 8, batch: 301, loss: 0.00156, accu: 0.95227, speed: 1.08 step/s\n",
      "global step 3230, epoch: 8, batch: 311, loss: 0.00478, accu: 0.95235, speed: 1.03 step/s\n",
      "global step 3240, epoch: 8, batch: 321, loss: 0.01479, accu: 0.95246, speed: 1.13 step/s\n",
      "global step 3250, epoch: 8, batch: 331, loss: 0.00132, accu: 0.95257, speed: 1.01 step/s\n",
      "global step 3260, epoch: 8, batch: 341, loss: 0.00245, accu: 0.95269, speed: 1.00 step/s\n",
      "global step 3270, epoch: 8, batch: 351, loss: 0.00264, accu: 0.95280, speed: 1.01 step/s\n",
      "global step 3280, epoch: 8, batch: 361, loss: 0.00031, accu: 0.95288, speed: 1.10 step/s\n",
      "global step 3290, epoch: 8, batch: 371, loss: 0.00437, accu: 0.95295, speed: 1.20 step/s\n",
      "global step 3300, epoch: 8, batch: 381, loss: 0.00528, accu: 0.95303, speed: 0.99 step/s\n",
      "global step 3310, epoch: 8, batch: 391, loss: 0.01017, accu: 0.95312, speed: 1.14 step/s\n",
      "global step 3320, epoch: 8, batch: 401, loss: 0.00261, accu: 0.95321, speed: 1.10 step/s\n",
      "global step 3330, epoch: 8, batch: 411, loss: 0.01310, accu: 0.95331, speed: 1.10 step/s\n",
      "global step 3340, epoch: 9, batch: 4, loss: 0.00713, accu: 0.95340, speed: 1.08 step/s\n",
      "global step 3350, epoch: 9, batch: 14, loss: 0.00218, accu: 0.95351, speed: 1.08 step/s\n",
      "global step 3360, epoch: 9, batch: 24, loss: 0.00257, accu: 0.95359, speed: 1.09 step/s\n",
      "global step 3370, epoch: 9, batch: 34, loss: 0.01364, accu: 0.95368, speed: 1.06 step/s\n",
      "global step 3380, epoch: 9, batch: 44, loss: 0.03765, accu: 0.95379, speed: 0.94 step/s\n",
      "global step 3390, epoch: 9, batch: 54, loss: 0.03830, accu: 0.95388, speed: 0.99 step/s\n",
      "global step 3400, epoch: 9, batch: 64, loss: 0.00977, accu: 0.95398, speed: 1.09 step/s\n",
      "global step 3410, epoch: 9, batch: 74, loss: 0.00074, accu: 0.95408, speed: 1.23 step/s\n",
      "global step 3420, epoch: 9, batch: 84, loss: 0.00057, accu: 0.95416, speed: 1.23 step/s\n",
      "global step 3430, epoch: 9, batch: 94, loss: 0.02349, accu: 0.95426, speed: 1.00 step/s\n",
      "global step 3440, epoch: 9, batch: 104, loss: 0.00660, accu: 0.95435, speed: 1.04 step/s\n",
      "global step 3450, epoch: 9, batch: 114, loss: 0.00059, accu: 0.95445, speed: 1.28 step/s\n",
      "global step 3460, epoch: 9, batch: 124, loss: 0.00031, accu: 0.95455, speed: 1.23 step/s\n",
      "global step 3470, epoch: 9, batch: 134, loss: 0.01423, accu: 0.95465, speed: 1.04 step/s\n",
      "global step 3480, epoch: 9, batch: 144, loss: 0.00194, accu: 0.95473, speed: 1.14 step/s\n",
      "global step 3490, epoch: 9, batch: 154, loss: 0.00020, accu: 0.95482, speed: 1.04 step/s\n",
      "global step 3500, epoch: 9, batch: 164, loss: 0.04936, accu: 0.95490, speed: 1.07 step/s\n",
      "global step 3510, epoch: 9, batch: 174, loss: 0.00056, accu: 0.95497, speed: 1.03 step/s\n",
      "global step 3520, epoch: 9, batch: 184, loss: 0.00119, accu: 0.95506, speed: 1.39 step/s\n",
      "global step 3530, epoch: 9, batch: 194, loss: 0.00055, accu: 0.95515, speed: 1.06 step/s\n",
      "global step 3540, epoch: 9, batch: 204, loss: 0.00085, accu: 0.95524, speed: 1.19 step/s\n",
      "global step 3550, epoch: 9, batch: 214, loss: 0.00199, accu: 0.95532, speed: 1.10 step/s\n",
      "global step 3560, epoch: 9, batch: 224, loss: 0.02031, accu: 0.95542, speed: 1.13 step/s\n",
      "global step 3570, epoch: 9, batch: 234, loss: 0.00115, accu: 0.95552, speed: 1.13 step/s\n",
      "global step 3580, epoch: 9, batch: 244, loss: 0.06702, accu: 0.95561, speed: 1.09 step/s\n",
      "global step 3590, epoch: 9, batch: 254, loss: 0.00060, accu: 0.95570, speed: 1.18 step/s\n",
      "global step 3600, epoch: 9, batch: 264, loss: 0.00390, accu: 0.95576, speed: 1.01 step/s\n",
      "global step 3610, epoch: 9, batch: 274, loss: 0.09333, accu: 0.95582, speed: 1.04 step/s\n",
      "global step 3620, epoch: 9, batch: 284, loss: 0.00122, accu: 0.95589, speed: 1.04 step/s\n",
      "global step 3630, epoch: 9, batch: 294, loss: 0.00138, accu: 0.95598, speed: 1.01 step/s\n",
      "global step 3640, epoch: 9, batch: 304, loss: 0.01165, accu: 0.95607, speed: 0.96 step/s\n",
      "global step 3650, epoch: 9, batch: 314, loss: 0.01976, accu: 0.95614, speed: 1.00 step/s\n",
      "global step 3660, epoch: 9, batch: 324, loss: 0.00468, accu: 0.95622, speed: 0.99 step/s\n",
      "global step 3670, epoch: 9, batch: 334, loss: 0.05724, accu: 0.95629, speed: 1.25 step/s\n",
      "global step 3680, epoch: 9, batch: 344, loss: 0.00121, accu: 0.95635, speed: 1.19 step/s\n",
      "global step 3690, epoch: 9, batch: 354, loss: 0.00247, accu: 0.95641, speed: 1.06 step/s\n",
      "global step 3700, epoch: 9, batch: 364, loss: 0.01991, accu: 0.95649, speed: 1.07 step/s\n",
      "global step 3710, epoch: 9, batch: 374, loss: 0.00100, accu: 0.95659, speed: 1.11 step/s\n",
      "global step 3720, epoch: 9, batch: 384, loss: 0.01855, accu: 0.95667, speed: 1.00 step/s\n",
      "global step 3730, epoch: 9, batch: 394, loss: 0.00263, accu: 0.95676, speed: 1.08 step/s\n",
      "global step 3740, epoch: 9, batch: 404, loss: 0.02754, accu: 0.95684, speed: 1.19 step/s\n",
      "global step 3750, epoch: 9, batch: 414, loss: 0.00844, accu: 0.95693, speed: 1.05 step/s\n",
      "global step 3760, epoch: 10, batch: 7, loss: 0.00883, accu: 0.95701, speed: 1.11 step/s\n",
      "global step 3770, epoch: 10, batch: 17, loss: 0.00038, accu: 0.95709, speed: 1.04 step/s\n",
      "global step 3780, epoch: 10, batch: 27, loss: 0.00177, accu: 0.95718, speed: 1.12 step/s\n",
      "global step 3790, epoch: 10, batch: 37, loss: 0.06347, accu: 0.95726, speed: 1.20 step/s\n",
      "global step 3800, epoch: 10, batch: 47, loss: 0.00427, accu: 0.95734, speed: 1.02 step/s\n",
      "global step 3810, epoch: 10, batch: 57, loss: 0.00279, accu: 0.95742, speed: 1.17 step/s\n",
      "global step 3820, epoch: 10, batch: 67, loss: 0.00109, accu: 0.95751, speed: 1.08 step/s\n",
      "global step 3830, epoch: 10, batch: 77, loss: 0.01060, accu: 0.95759, speed: 0.99 step/s\n",
      "global step 3840, epoch: 10, batch: 87, loss: 0.02975, accu: 0.95767, speed: 1.07 step/s\n",
      "global step 3850, epoch: 10, batch: 97, loss: 0.00067, accu: 0.95776, speed: 1.16 step/s\n",
      "global step 3860, epoch: 10, batch: 107, loss: 0.00019, accu: 0.95785, speed: 1.12 step/s\n",
      "global step 3870, epoch: 10, batch: 117, loss: 0.00086, accu: 0.95793, speed: 1.03 step/s\n",
      "global step 3880, epoch: 10, batch: 127, loss: 0.00377, accu: 0.95802, speed: 1.22 step/s\n",
      "global step 3890, epoch: 10, batch: 137, loss: 0.00035, accu: 0.95811, speed: 1.06 step/s\n",
      "global step 3900, epoch: 10, batch: 147, loss: 0.00011, accu: 0.95818, speed: 1.09 step/s\n",
      "global step 3910, epoch: 10, batch: 157, loss: 0.00036, accu: 0.95827, speed: 1.14 step/s\n",
      "global step 3920, epoch: 10, batch: 167, loss: 0.00006, accu: 0.95835, speed: 1.12 step/s\n",
      "global step 3930, epoch: 10, batch: 177, loss: 0.00127, accu: 0.95843, speed: 1.13 step/s\n",
      "global step 3940, epoch: 10, batch: 187, loss: 0.00008, accu: 0.95850, speed: 1.16 step/s\n",
      "global step 3950, epoch: 10, batch: 197, loss: 0.00035, accu: 0.95857, speed: 1.24 step/s\n",
      "global step 3960, epoch: 10, batch: 207, loss: 0.00110, accu: 0.95865, speed: 1.06 step/s\n",
      "global step 3970, epoch: 10, batch: 217, loss: 0.00117, accu: 0.95872, speed: 1.04 step/s\n",
      "global step 3980, epoch: 10, batch: 227, loss: 0.00995, accu: 0.95881, speed: 1.06 step/s\n",
      "global step 3990, epoch: 10, batch: 237, loss: 0.00700, accu: 0.95889, speed: 1.16 step/s\n",
      "global step 4000, epoch: 10, batch: 247, loss: 0.00091, accu: 0.95894, speed: 1.12 step/s\n",
      "global step 4010, epoch: 10, batch: 257, loss: 0.00026, accu: 0.95899, speed: 0.41 step/s\n",
      "global step 4020, epoch: 10, batch: 267, loss: 0.00290, accu: 0.95908, speed: 1.17 step/s\n",
      "global step 4030, epoch: 10, batch: 277, loss: 0.00025, accu: 0.95915, speed: 1.22 step/s\n",
      "global step 4040, epoch: 10, batch: 287, loss: 0.00093, accu: 0.95922, speed: 1.06 step/s\n",
      "global step 4050, epoch: 10, batch: 297, loss: 0.00461, accu: 0.95930, speed: 1.04 step/s\n",
      "global step 4060, epoch: 10, batch: 307, loss: 0.00054, accu: 0.95938, speed: 1.06 step/s\n",
      "global step 4070, epoch: 10, batch: 317, loss: 0.00043, accu: 0.95946, speed: 1.00 step/s\n",
      "global step 4080, epoch: 10, batch: 327, loss: 0.00406, accu: 0.95953, speed: 1.10 step/s\n",
      "global step 4090, epoch: 10, batch: 337, loss: 0.14750, accu: 0.95959, speed: 1.09 step/s\n",
      "global step 4100, epoch: 10, batch: 347, loss: 0.01897, accu: 0.95966, speed: 1.04 step/s\n",
      "global step 4110, epoch: 10, batch: 357, loss: 0.00019, accu: 0.95972, speed: 1.11 step/s\n",
      "global step 4120, epoch: 10, batch: 367, loss: 0.00219, accu: 0.95980, speed: 1.09 step/s\n",
      "global step 4130, epoch: 10, batch: 377, loss: 0.00024, accu: 0.95987, speed: 1.03 step/s\n",
      "global step 4140, epoch: 10, batch: 387, loss: 0.22838, accu: 0.95993, speed: 1.10 step/s\n",
      "global step 4150, epoch: 10, batch: 397, loss: 0.00157, accu: 0.95996, speed: 1.00 step/s\n",
      "global step 4160, epoch: 10, batch: 407, loss: 0.00812, accu: 0.96003, speed: 1.15 step/s\n",
      "global step 4170, epoch: 10, batch: 417, loss: 0.04391, accu: 0.96009, speed: 1.10 step/s\n"
     ]
    }
   ],
   "source": [
    "# 开启训练\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        # 喂数据给model\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(logits, labels)\n",
    "        # 预测分类概率值\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # 计算acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 1000 == 0:\n",
    "            save_dir = os.path.join(ckpt_dir, \"model_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            # 评估当前训练的模型\n",
    "            # evaluate(model, criterion, metric, dev_data_loader)\n",
    "            # 保存当前模型参数等\n",
    "            model.save_pretrained(save_dir)\n",
    "            # 保存tokenizer的词表等\n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "save_dir = os.path.join(ckpt_dir, \"model_final\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model.save_pretrained(save_dir)\n",
    "# 保存tokenizer的词表等\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "\n",
    "使用训练得到的模型还可以对文本进行情感预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "\n",
    "# 处理测试集数据\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_ckpt_NLPCC14-SC/model_final/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_ckpt_NLPCC14-SC/model_final/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([2], '0')\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: '0', 1: '1'}\n",
    "results = []\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\n",
    "model.eval()\n",
    "for batch in test_data_loader:\n",
    "    input_ids, token_type_ids, qids = batch\n",
    "    # 喂数据给模型\n",
    "    logits = model(input_ids, token_type_ids)\n",
    "    # 预测分类\n",
    "    probs = F.softmax(logits, axis=-1)\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\n",
    "    idx = idx.tolist()\n",
    "    labels = [label_map[i] for i in idx]\n",
    "    qids = qids.numpy().tolist()\n",
    "    results.extend(zip(qids, labels))\n",
    "print(results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "# 写入预测结果\n",
    "with open(os.path.join(res_dir, \"NLPCC14-SC.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for qid, label in results:\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part C.2 目标级情感分析\n",
    "\n",
    "在电商产品分析场景下，除了分析整体商品的情感极性外，还细化到以商品具体的“方面”为分析主体进行情感分析（aspect-level），如下、：\n",
    "\n",
    "* 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的**口味方面**是一个负向评价（咸，太辣），然而对于**口感方面**却是一个正向评价（很脆）。\n",
    "\n",
    "* 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于**夏威夷**是一个正向评价（喜欢），然而对于**夏威夷的海鲜**却是一个负向评价（价格太贵）。\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/052d46409ba3451693a718552b968d188fa4677235bc43ddbc15fe11ad3b57b1\" width=\"600\" ></center>\n",
    "<br><center>目标级情感分析任务</center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 常用数据集\n",
    "\n",
    "[千言数据集](https://www.luge.ai/)已提供了许多任务常用数据集。\n",
    "其中情感分析数据集下载链接：https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE\n",
    "\n",
    "SE-ABSA16_PHNS数据集是关于手机的目标级情感分析数据集。PaddleNLP已经内置了该数据集，加载方式，如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio\n",
      "--2021-06-18 17:19:25--  https://dataset-bj.cdn.bcebos.com/qianyan/SE-ABSA16_CAME.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 182.61.128.166\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|182.61.128.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 373019 (364K) [application/zip]\n",
      "Saving to: ‘/home/aistudio/data/SE-ABSA16_CAME.zip’\n",
      "\n",
      "/home/aistudio/data 100%[===================>] 364.28K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2021-06-18 17:19:25 (19.4 MB/s) - ‘/home/aistudio/data/SE-ABSA16_CAME.zip’ saved [373019/373019]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\r\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/SE-ABSA16_CAME.zip -O ~/data/SE-ABSA16_CAME.zip\r\n",
    "!unzip -oq /home/aistudio/data/SE-ABSA16_CAME.zip -d ~/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'camera#design_features', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。', 'label': '0'}\n",
      "{'text': 'camera#quality', 'text_pair': '一直潜水，昨天入d300s +35 1.8g，谈谈感受，dx说，标题一定要长！在我们这尼康一个代理商开的大型体验中心提的货，老板和销售mm都很热情，不欺诈，也没有店大欺客，mm很热情，从d300s到d800，d7000，到d3x配各种镜头，全部把玩了一番，感叹啊，真他妈好东西！尤其d3x，有钱了，一定要他妈买一个，还有，就是d800，一摸心中的神机，顿时凉了半截，可能摸她之前，摸了她们的头牌，d3x的缘故，这手感 真是差了点，样子嘛，之所以喜欢尼康，就是喜欢棱角分明的感觉，d3x方方正正 ，甚是讨喜，d800这丫头，变得圆滑了不少，不喜欢。都说电子产品，买新不买旧，我倒不认为这么看，中低端产品的确如此，但顶级的高端产品，真不是这么回事啊，d3x也是51点对焦，我的d300s也是51点，但明显感觉，对焦就是比d300s 快，准，暗部反差较小时，也很少拉风箱，我的d300s就不行，光线不好反差较小，拉回来拉过去，半天合不上焦，说真的，一分价钱一分货啊，d800电子性能 肯定是先进的，但机械性能 跟d3x还是没可比性，传感器固然先进，但三千多万 像素 和两千多万像素 对我们来说，真的差别这么大吗？d800e3万多，有这钱真的不如加点买 d3x啊，真要是d3x烂，为什么尼康不停产了？人说高像素 是给商业摄影师用，我们的音乐老师，是业余的音乐制作人，也拍摄一些商业广告，平时他玩的时候 都是数码什么的，nc 加起来十几个，大三元全都配齐，但干活的时候，还是120的机器，照他那话说，数码 像素太低，不够用啊！废话说太多了，谈谈感受吧，当初一直在纠结d7000和d300s，都说什么d7000画质超越d300s，我也信，但昨天拿到实机后，我瞬间就决定 d300s了，我的手算小的，握住d300s，我感觉，刚刚好，而且手柄凹槽 ，我觉得还不够深，握感不是十分的充盈，这点要像宾得k5学习，而且d7000小了一点，背部操作空间局促，大拇指没地放，果断d300s，而且试机的时候，我给d300s 换上了24-70，可能我练健身比较久了，没感觉有啥重量，蛮趁手的，现在配35 1.8 感觉轻飘飘的，哈哈，', 'label': '0'}\n"
     ]
    }
   ],
   "source": [
    "# from paddlenlp.datasets import load_dataset\n",
    "# train_ds, test_ds = load_dataset(\"seabsa16\", \"phns\", splits=[\"train\", \"test\"])\n",
    "# train_ds, test_ds = load_dataset(\"seabsa16\", \"came\", splits=[\"train\", \"test\"])\n",
    "from paddlenlp.datasets import MapDataset\n",
    "# train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# train_ds, dev_ds, test_ds = load_dataset(\"ChnSentiCorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            # next(fp)  # Skip header\n",
    "            cnt=0\n",
    "            title_list = fp.readline().strip('\\n').split('\\t')\n",
    "            for line in fp.readlines():\n",
    "                if 'test' in data_path:\n",
    "                    label, text, text_pair = line.strip('\\n').split('\\t')\n",
    "                if 'train' in data_path:\n",
    "                    label, text, text_pair = line.strip('\\n').split('\\t')\n",
    "                # words = words.split('\\002')\n",
    "                # labels = labels.split('\\002')\n",
    "                #  {'text': text, 'label': labels, 'qid':''}\n",
    "                yield {'text': text, 'text_pair':text_pair, 'label':label}\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds,  test_ds = load_dataset(datafiles=(\n",
    "        './data/SE-ABSA16_CAME/train.tsv', './data/SE-ABSA16_CAME/test.tsv'))\n",
    "# train_ds = load_dataset(datafiles=('./data/ChnSentiCorp/dev.tsv'))\n",
    "print(train_ds[0])\n",
    "print(test_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1']\n",
      "{'text': 'phone#design_features', 'text_pair': '今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化，长度多了大概一厘米，也就是之前所说的多了一排的图标。2. 真机重量比上一代轻了很多，个人感觉跟i9100的重量差不多。（用惯上一代的朋友可能需要一段时间适应了）3. 由于目前还没有版的SIM卡，无法插卡使用，有购买的朋友要注意了，并非简单的剪卡就可以用，而是需要去运营商更换新一代的SIM卡。4. 屏幕显示效果确实比上一代有进步，不论是从清晰度还是不同角度的视角，iPhone 5绝对要更上一层，我想这也许是相对上一代最有意义的升级了。5. 新的数据接口更小，比上一代更好用更方便，使用的过程会有这样的体会。6. 从简单的几个操作来讲速度比4s要快，这个不用测试软件也能感受出来，比如程序的调用以及照片的拍摄和浏览。不过，目前水货市场上坑爹的价格，最好大家可以再观望一下，不要急着出手。', 'label': 1}\n",
      "{'text': 'software#usability', 'text_pair': '刚刚入手8600，体会。刚刚从淘宝购买，1635元（包邮）。1、全新，应该是欧版机，配件也是正品全新。2、在三星官网下载了KIES，可用免费软件非常多，绝对够用。3、不到2000元能买到此种手机，知足了。'}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.label_list)\r\n",
    "print(train_ds[0])\r\n",
    "print(test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SKEP模型加载\n",
    "\n",
    "目标级情感分析模型同样使用`SkepForSequenceClassification`模型，但目标级情感分析模型的输入不单单是一个句子，而是句对。一个句子描述“评价对象方面（aspect）”，另一个句子描述\"对该方面的评论\"。如下图所示。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a4b76447dae404caa3bf123ea28e375179cb09a02de4bef8a2f172edc6e3c8f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-18 17:30:16,517] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-18 17:30:26,120] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\n",
    "# 指定模型名称一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=2)\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始SE_ABSA16_PHNS数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "\n",
    "def convert_example(example,\n",
    "                    tokenizer,\n",
    "                    max_seq_length=512,\n",
    "                    is_test=False,\n",
    "                    dataset_name=\"chnsenticorp\"):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "    by concatenating and adding special tokens. And creates a mask from the two sequences passed \n",
    "    to be used in a sequence-pair classification task.\n",
    "        \n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence has the following format:\n",
    "    ::\n",
    "        - single sequence: ``[CLS] X [SEP]``\n",
    "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
    "\n",
    "    A skep_ernie_1.0_large_ch/skep_ernie_2.0_large_en sequence pair mask has the following format:\n",
    "    ::\n",
    "\n",
    "        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence |\n",
    "\n",
    "    If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).\n",
    "    \n",
    "    note: There is no need token type ids for skep_roberta_large_ch model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        tokenizer(obj:`PretrainedTokenizer`): This tokenizer inherits from :class:`~paddlenlp.transformers.PretrainedTokenizer` \n",
    "            which contains most of the methods. Users should refer to the superclass for more information regarding methods.\n",
    "        max_seq_len(obj:`int`): The maximum total input sequence length after tokenization. \n",
    "            Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "        dataset_name((obj:`str`, defaults to \"chnsenticorp\"): The dataset name, \"chnsenticorp\" or \"sst-2\".\n",
    "\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.\n",
    "        token_type_ids(obj: `list[int]`): List of sequence pair mask.\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=example[\"text\"],\n",
    "        text_pair=example[\"text_pair\"],\n",
    "        max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import create_dataloader\n",
    "# 处理的最大文本序列长度\n",
    "max_seq_length=256\n",
    "# 批量数据大小\n",
    "batch_size=16\n",
    "\n",
    "# 将数据处理成model可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "# 优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    parameters=model.parameters())\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# Accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.81712, acc: 0.60625, speed: 1.24 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.67679, acc: 0.59062, speed: 1.26 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.67554, acc: 0.60417, speed: 1.27 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.65711, acc: 0.59844, speed: 1.28 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.75251, acc: 0.59750, speed: 1.25 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.57564, acc: 0.61146, speed: 1.24 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.57873, acc: 0.61786, speed: 1.25 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.72466, acc: 0.62969, speed: 1.30 step/s\n",
      "global step 90, epoch: 2, batch: 7, loss: 0.49440, acc: 0.63681, speed: 1.30 step/s\n",
      "global step 100, epoch: 2, batch: 17, loss: 0.50333, acc: 0.64317, speed: 1.26 step/s\n",
      "global step 110, epoch: 2, batch: 27, loss: 0.62377, acc: 0.64723, speed: 1.25 step/s\n",
      "global step 120, epoch: 2, batch: 37, loss: 0.57376, acc: 0.65322, speed: 1.25 step/s\n",
      "global step 130, epoch: 2, batch: 47, loss: 0.83734, acc: 0.65732, speed: 1.26 step/s\n",
      "global step 140, epoch: 2, batch: 57, loss: 0.53514, acc: 0.65814, speed: 1.25 step/s\n",
      "global step 150, epoch: 2, batch: 67, loss: 0.73370, acc: 0.66053, speed: 1.25 step/s\n",
      "global step 160, epoch: 2, batch: 77, loss: 0.61803, acc: 0.66418, speed: 1.24 step/s\n",
      "global step 170, epoch: 3, batch: 4, loss: 0.61744, acc: 0.66568, speed: 1.33 step/s\n",
      "global step 180, epoch: 3, batch: 14, loss: 0.51311, acc: 0.67075, speed: 1.26 step/s\n",
      "global step 190, epoch: 3, batch: 24, loss: 0.68197, acc: 0.67197, speed: 1.25 step/s\n",
      "global step 200, epoch: 3, batch: 34, loss: 0.52004, acc: 0.67306, speed: 1.20 step/s\n",
      "global step 210, epoch: 3, batch: 44, loss: 0.70952, acc: 0.67406, speed: 1.23 step/s\n",
      "global step 220, epoch: 3, batch: 54, loss: 0.48056, acc: 0.67467, speed: 1.26 step/s\n",
      "global step 230, epoch: 3, batch: 64, loss: 0.78385, acc: 0.67633, speed: 1.23 step/s\n",
      "global step 240, epoch: 3, batch: 74, loss: 0.83196, acc: 0.67548, speed: 1.24 step/s\n",
      "global step 250, epoch: 4, batch: 1, loss: 0.61993, acc: 0.67532, speed: 1.32 step/s\n",
      "global step 260, epoch: 4, batch: 11, loss: 0.51936, acc: 0.67652, speed: 1.25 step/s\n",
      "global step 270, epoch: 4, batch: 21, loss: 0.49737, acc: 0.67670, speed: 1.24 step/s\n",
      "global step 280, epoch: 4, batch: 31, loss: 0.69898, acc: 0.67664, speed: 1.25 step/s\n",
      "global step 290, epoch: 4, batch: 41, loss: 0.45895, acc: 0.67549, speed: 1.23 step/s\n",
      "global step 300, epoch: 4, batch: 51, loss: 0.57667, acc: 0.67716, speed: 1.24 step/s\n",
      "global step 310, epoch: 4, batch: 61, loss: 0.39892, acc: 0.68114, speed: 1.24 step/s\n",
      "global step 320, epoch: 4, batch: 71, loss: 0.46283, acc: 0.68410, speed: 1.23 step/s\n",
      "global step 330, epoch: 4, batch: 81, loss: 0.76162, acc: 0.68420, speed: 1.28 step/s\n",
      "global step 340, epoch: 5, batch: 8, loss: 0.71802, acc: 0.68328, speed: 1.28 step/s\n",
      "global step 350, epoch: 5, batch: 18, loss: 0.59283, acc: 0.68341, speed: 1.22 step/s\n",
      "global step 360, epoch: 5, batch: 28, loss: 0.44692, acc: 0.68474, speed: 1.22 step/s\n",
      "global step 370, epoch: 5, batch: 38, loss: 0.49606, acc: 0.68669, speed: 1.25 step/s\n",
      "global step 380, epoch: 5, batch: 48, loss: 0.80795, acc: 0.68754, speed: 1.24 step/s\n",
      "global step 390, epoch: 5, batch: 58, loss: 0.63275, acc: 0.68899, speed: 1.25 step/s\n",
      "global step 400, epoch: 5, batch: 68, loss: 0.50945, acc: 0.68738, speed: 1.23 step/s\n",
      "global step 410, epoch: 5, batch: 78, loss: 0.48268, acc: 0.68892, speed: 1.24 step/s\n",
      "global step 420, epoch: 6, batch: 5, loss: 0.43505, acc: 0.68957, speed: 1.31 step/s\n",
      "global step 430, epoch: 6, batch: 15, loss: 0.69845, acc: 0.69099, speed: 1.23 step/s\n",
      "global step 440, epoch: 6, batch: 25, loss: 0.66922, acc: 0.69248, speed: 1.25 step/s\n",
      "global step 450, epoch: 6, batch: 35, loss: 0.59764, acc: 0.69237, speed: 1.24 step/s\n",
      "global step 460, epoch: 6, batch: 45, loss: 0.53675, acc: 0.69227, speed: 1.25 step/s\n",
      "global step 470, epoch: 6, batch: 55, loss: 0.44329, acc: 0.69203, speed: 1.22 step/s\n",
      "global step 480, epoch: 6, batch: 65, loss: 0.61324, acc: 0.69207, speed: 1.24 step/s\n",
      "global step 490, epoch: 6, batch: 75, loss: 0.59884, acc: 0.69261, speed: 1.24 step/s\n",
      "global step 500, epoch: 7, batch: 2, loss: 0.48194, acc: 0.69196, speed: 1.32 step/s\n",
      "global step 510, epoch: 7, batch: 12, loss: 0.54687, acc: 0.69199, speed: 0.46 step/s\n",
      "global step 520, epoch: 7, batch: 22, loss: 0.77207, acc: 0.69288, speed: 1.22 step/s\n",
      "global step 530, epoch: 7, batch: 32, loss: 0.85220, acc: 0.69242, speed: 1.26 step/s\n",
      "global step 540, epoch: 7, batch: 42, loss: 0.67257, acc: 0.69233, speed: 1.26 step/s\n",
      "global step 550, epoch: 7, batch: 52, loss: 0.46508, acc: 0.69292, speed: 1.24 step/s\n",
      "global step 560, epoch: 7, batch: 62, loss: 0.54035, acc: 0.69384, speed: 1.24 step/s\n",
      "global step 570, epoch: 7, batch: 72, loss: 0.44571, acc: 0.69450, speed: 1.26 step/s\n",
      "global step 580, epoch: 7, batch: 82, loss: 0.33537, acc: 0.69568, speed: 1.30 step/s\n",
      "global step 590, epoch: 8, batch: 9, loss: 0.95964, acc: 0.69636, speed: 1.29 step/s\n",
      "global step 600, epoch: 8, batch: 19, loss: 0.74278, acc: 0.69684, speed: 1.24 step/s\n",
      "global step 610, epoch: 8, batch: 29, loss: 0.58175, acc: 0.69668, speed: 1.25 step/s\n",
      "global step 620, epoch: 8, batch: 39, loss: 0.41799, acc: 0.69704, speed: 1.23 step/s\n",
      "global step 630, epoch: 8, batch: 49, loss: 0.52126, acc: 0.69749, speed: 1.24 step/s\n",
      "global step 640, epoch: 8, batch: 59, loss: 0.49854, acc: 0.69733, speed: 1.24 step/s\n",
      "global step 650, epoch: 8, batch: 69, loss: 0.51916, acc: 0.69689, speed: 1.23 step/s\n",
      "global step 660, epoch: 8, batch: 79, loss: 0.49958, acc: 0.69818, speed: 1.26 step/s\n",
      "global step 670, epoch: 9, batch: 6, loss: 0.36081, acc: 0.69930, speed: 1.31 step/s\n",
      "global step 680, epoch: 9, batch: 16, loss: 0.52940, acc: 0.69978, speed: 1.26 step/s\n",
      "global step 690, epoch: 9, batch: 26, loss: 0.74897, acc: 0.70079, speed: 1.24 step/s\n",
      "global step 700, epoch: 9, batch: 36, loss: 0.55542, acc: 0.70077, speed: 1.24 step/s\n",
      "global step 710, epoch: 9, batch: 46, loss: 0.54171, acc: 0.70041, speed: 1.24 step/s\n",
      "global step 720, epoch: 9, batch: 56, loss: 0.62070, acc: 0.70066, speed: 1.23 step/s\n",
      "global step 730, epoch: 9, batch: 66, loss: 0.66615, acc: 0.70057, speed: 1.22 step/s\n",
      "global step 740, epoch: 9, batch: 76, loss: 0.80069, acc: 0.70124, speed: 1.23 step/s\n",
      "global step 750, epoch: 10, batch: 3, loss: 0.53136, acc: 0.70229, speed: 1.33 step/s\n",
      "global step 760, epoch: 10, batch: 13, loss: 0.73243, acc: 0.70243, speed: 1.24 step/s\n",
      "global step 770, epoch: 10, batch: 23, loss: 0.63415, acc: 0.70248, speed: 1.25 step/s\n",
      "global step 780, epoch: 10, batch: 33, loss: 0.62445, acc: 0.70293, speed: 1.22 step/s\n",
      "global step 790, epoch: 10, batch: 43, loss: 0.51380, acc: 0.70409, speed: 1.24 step/s\n",
      "global step 800, epoch: 10, batch: 53, loss: 0.61747, acc: 0.70396, speed: 1.25 step/s\n",
      "global step 810, epoch: 10, batch: 63, loss: 0.44841, acc: 0.70344, speed: 1.24 step/s\n",
      "global step 820, epoch: 10, batch: 73, loss: 0.44397, acc: 0.70348, speed: 1.23 step/s\n",
      "global step 830, epoch: 10, batch: 83, loss: 0.70255, acc: 0.70342, speed: 1.33 step/s\n"
     ]
    }
   ],
   "source": [
    "# 开启训练\n",
    "ckpt_dir = \"skep_aspect\"\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        # 喂数据给model\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        # 计算损失函数值\n",
    "        loss = criterion(logits, labels)\n",
    "        # 预测分类概率\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        # 计算acc\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # 反向梯度回传，更新参数\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if global_step % 500 == 0:\n",
    "            \n",
    "            save_dir = os.path.join(ckpt_dir, \"modelCAME_%d\" % global_step)\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            # 保存模型参数\n",
    "            model.save_pretrained(save_dir)\n",
    "            # 保存tokenizer的词表等\n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "save_dir = os.path.join(ckpt_dir, \"modelCAME_final\")\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "# 保存模型参数\n",
    "model.save_pretrained(save_dir)\n",
    "# 保存tokenizer的词表等\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "使用训练得到的模型还可以对评价对象进行情感预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def predict(model, data_loader, label_map):\n",
    "    \"\"\"\n",
    "    Given a prediction dataset, it gives the prediction results.\n",
    "\n",
    "    Args:\n",
    "        model(obj:`paddle.nn.Layer`): A model to classify texts.\n",
    "        data_loader(obj:`paddle.io.DataLoader`): The dataset loader which generates batches.\n",
    "        label_map(obj:`dict`): The label id (key) to label str (value) map.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "label_map = {0: '0', 1: '1'}\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_ckpt/modelCAME_final/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = predict(model, test_data_loader, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "with open(os.path.join(\"results\", \"SE-ABSA16_CAME.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "将预测文件结果压缩至zip文件，提交[千言比赛网站](https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE)\n",
    "\n",
    "**NOTE:** results文件夹中NLPCC14-SC.tsv、SE-ABSA16_CAME.tsv、COTE_BD.tsv、COTE_MFW.tsv、COTE_DP.tsv等文件是为了顺利提交，补齐的文件。\n",
    "其结果还有待提高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/ (stored 0%)\r\n",
      "  adding: results/SE-ABSA16_PHNS.tsv (deflated 65%)\r\n",
      "  adding: results/ChnSentiCorp.tsv (deflated 63%)\r\n"
     ]
    }
   ],
   "source": [
    "#将预测文件结果压缩至zip文件，提交\n",
    "!zip -r results.zip results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part D 观点抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp\r\n",
    "from paddlenlp.transformers import SkepForTokenClassification, SkepTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-23 11:28:01--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-BD.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 182.61.128.166\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|182.61.128.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1182741 (1.1M) [application/zip]\n",
      "Saving to: ‘/home/aistudio/data/COTE-BD.zip’\n",
      "\n",
      "/home/aistudio/data 100%[===================>]   1.13M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2021-06-23 11:28:01 (39.9 MB/s) - ‘/home/aistudio/data/COTE-BD.zip’ saved [1182741/1182741]\n",
      "\n",
      "--2021-06-23 11:28:01--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-MFW.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 182.61.128.166\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|182.61.128.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4872264 (4.6M) [application/zip]\n",
      "Saving to: ‘/home/aistudio/data/COTE-MFW.zip’\n",
      "\n",
      "/home/aistudio/data 100%[===================>]   4.65M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2021-06-23 11:28:02 (45.1 MB/s) - ‘/home/aistudio/data/COTE-MFW.zip’ saved [4872264/4872264]\n",
      "\n",
      "--2021-06-23 11:28:02--  https://dataset-bj.cdn.bcebos.com/qianyan/COTE-DP.zip\n",
      "Resolving dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)... 182.61.128.166\n",
      "Connecting to dataset-bj.cdn.bcebos.com (dataset-bj.cdn.bcebos.com)|182.61.128.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4220083 (4.0M) [application/zip]\n",
      "Saving to: ‘/home/aistudio/data/COTE-DP.zip’\n",
      "\n",
      "/home/aistudio/data 100%[===================>]   4.02M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2021-06-23 11:28:02 (47.3 MB/s) - ‘/home/aistudio/data/COTE-DP.zip’ saved [4220083/4220083]\n",
      "\n",
      "Archive:  data/COTE-BD.zip\n",
      "   creating: COTE-BD/\n",
      "  inflating: COTE-BD/train.tsv       \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/COTE-BD/\n",
      "  inflating: __MACOSX/COTE-BD/._train.tsv  \n",
      "  inflating: COTE-BD/License.pdf     \n",
      "  inflating: __MACOSX/COTE-BD/._License.pdf  \n",
      "  inflating: COTE-BD/test.tsv        \n",
      "  inflating: __MACOSX/COTE-BD/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-BD      \n",
      "Archive:  data/COTE-DP.zip\n",
      "   creating: COTE-DP/\n",
      "  inflating: COTE-DP/train.tsv       \n",
      "   creating: __MACOSX/COTE-DP/\n",
      "  inflating: __MACOSX/COTE-DP/._train.tsv  \n",
      "  inflating: COTE-DP/License.pdf     \n",
      "  inflating: __MACOSX/COTE-DP/._License.pdf  \n",
      "  inflating: COTE-DP/test.tsv        \n",
      "  inflating: __MACOSX/COTE-DP/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-DP      \n",
      "Archive:  data/COTE-MFW.zip\n",
      "   creating: COTE-MFW/\n",
      "  inflating: COTE-MFW/train.tsv      \n",
      "   creating: __MACOSX/COTE-MFW/\n",
      "  inflating: __MACOSX/COTE-MFW/._train.tsv  \n",
      "  inflating: COTE-MFW/License.pdf    \n",
      "  inflating: __MACOSX/COTE-MFW/._License.pdf  \n",
      "  inflating: COTE-MFW/test.tsv       \n",
      "  inflating: __MACOSX/COTE-MFW/._test.tsv  \n",
      "  inflating: __MACOSX/._COTE-MFW     \n"
     ]
    }
   ],
   "source": [
    "# 导入数据\r\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-BD.zip -O ~/data/COTE-BD.zip\r\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-MFW.zip -O ~/data/COTE-MFW.zip\r\n",
    "!wget https://dataset-bj.cdn.bcebos.com/qianyan/COTE-DP.zip -O ~/data/COTE-DP.zip\r\n",
    "\r\n",
    "\r\n",
    "# 解压数据\r\n",
    "!unzip -o data/COTE-BD\r\n",
    "!unzip -o data/COTE-DP\r\n",
    "!unzip -o data/COTE-MFW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 数据集字典\r\n",
    "def open_func(file_path):\r\n",
    "    return [line.strip() for line in open(file_path, 'r', encoding='utf8').readlines()[1:] if len(line.strip().split('\\t')) >= 2]\r\n",
    "\r\n",
    "data_dict = {'cotebd': {'test': open_func('COTE-BD/test.tsv'),\r\n",
    "                        'train': open_func('COTE-BD/train.tsv')},\r\n",
    "             'cotedp': {'test': open_func('COTE-DP/test.tsv'),\r\n",
    "                        'train': open_func('COTE-DP/train.tsv')},\r\n",
    "             'cotemfw': {'test': open_func('COTE-MFW/test.tsv'),\r\n",
    "                        'train': open_func('COTE-MFW/train.tsv')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据集\r\n",
    "from paddle.io import Dataset, DataLoader\r\n",
    "from paddlenlp.data import Pad, Stack, Tuple\r\n",
    "import numpy as np\r\n",
    "label_list = {'B': 0, 'I': 1, 'O': 2}\r\n",
    "index2label = {0: 'B', 1: 'I', 2: 'O'}\r\n",
    "\r\n",
    "# 考虑token_type_id\r\n",
    "class MyDataset(Dataset):\r\n",
    "    def __init__(self, data, tokenizer, max_len=512, for_test=False):\r\n",
    "        super().__init__()\r\n",
    "        self._data = data\r\n",
    "        self._tokenizer = tokenizer\r\n",
    "        self._max_len = max_len\r\n",
    "        self._for_test = for_test\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return len(self._data)\r\n",
    "    \r\n",
    "    def __getitem__(self, idx):\r\n",
    "        samples = self._data[idx].split('\\t')\r\n",
    "        label = samples[-2]\r\n",
    "        text = samples[-1]\r\n",
    "        if self._for_test:\r\n",
    "            origin_enc = self._tokenizer.encode(text, max_seq_len=self._max_len)['input_ids']\r\n",
    "            return np.array(origin_enc, dtype='int64')\r\n",
    "        else:\r\n",
    "            \r\n",
    "            # 由于并不是每个字都是一个token，这里采用一种简单的处理方法，先编码label，再编码text中除了label以外的词，最后合到一起\r\n",
    "            texts = text.split(label)\r\n",
    "            label_enc = self._tokenizer.encode(label)['input_ids']\r\n",
    "            cls_enc = label_enc[0]\r\n",
    "            sep_enc = label_enc[-1]\r\n",
    "            label_enc = label_enc[1:-1]\r\n",
    "            \r\n",
    "            # 合并\r\n",
    "            origin_enc = []\r\n",
    "            label_ids = []\r\n",
    "            for index, text in enumerate(texts):\r\n",
    "                text_enc = self._tokenizer.encode(text)['input_ids']\r\n",
    "                text_enc = text_enc[1:-1]\r\n",
    "                origin_enc += text_enc\r\n",
    "                label_ids += [label_list['O']] * len(text_enc)\r\n",
    "                if index != len(texts) - 1:\r\n",
    "                    origin_enc += label_enc\r\n",
    "                    label_ids += [label_list['B']] + [label_list['I']] * (len(label_enc) - 1)\r\n",
    "\r\n",
    "            origin_enc = [cls_enc] + origin_enc + [sep_enc]\r\n",
    "            label_ids = [label_list['O']] + label_ids + [label_list['O']]\r\n",
    "            \r\n",
    "            # 截断\r\n",
    "            if len(origin_enc) > self._max_len:\r\n",
    "                origin_enc = origin_enc[:self._max_len-1] + origin_enc[-1:]\r\n",
    "                label_ids = label_ids[:self._max_len-1] + label_ids[-1:]\r\n",
    "            return np.array(origin_enc, dtype='int64'), np.array(label_ids, dtype='int64')\r\n",
    "\r\n",
    "\r\n",
    "def batchify_fn(for_test=False):\r\n",
    "    if for_test:\r\n",
    "        return lambda samples, fn=Pad(axis=0, pad_val=tokenizer.pad_token_id): np.row_stack([data for data in fn(samples)])\r\n",
    "    else:\r\n",
    "        return lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id),\r\n",
    "                                        Pad(axis=0, pad_val=label_list['O'])): [data for data in fn(samples)]\r\n",
    "\r\n",
    "\r\n",
    "def get_data_loader(data, tokenizer, batch_size=32, max_len=512, for_test=False):\r\n",
    "    dataset = MyDataset(data, tokenizer, max_len, for_test)\r\n",
    "    shuffle = True if not for_test else False\r\n",
    "    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=batchify_fn(for_test))\r\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-23 13:29:43,003] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "[2021-06-23 13:29:47,936] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.static import InputSpec\r\n",
    "from paddlenlp.metrics import Perplexity\r\n",
    "\r\n",
    "# 模型和分词\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')\r\n",
    "\r\n",
    "# 参数设置\r\n",
    "data_path_name = 'cotedp'  # 更改此选项改变数据集\r\n",
    "# data_path_name = 'cotebd'  # 更改此选项改变数据集\r\n",
    "# data_path_name = 'cotemfw'  # 更改此选项改变数据集\r\n",
    "## 训练相关\r\n",
    "epochs = 2\r\n",
    "learning_rate = 2e-5\r\n",
    "batch_size = 8\r\n",
    "max_len = 512\r\n",
    "\r\n",
    "## 数据相关\r\n",
    "train_dataloader = get_data_loader(data_dict[data_path_name]['train'], tokenizer, batch_size, max_len, for_test=False)\r\n",
    "\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "label = InputSpec((-1, -1, 3), dtype='int64', name='label')\r\n",
    "model = paddle.Model(model, [input], [label])\r\n",
    "\r\n",
    "# 模型准备\r\n",
    "\r\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, parameters=model.parameters())\r\n",
    "model.prepare(optimizer, loss=paddle.nn.CrossEntropyLoss(), metrics=[Perplexity()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/2\n",
      "step  200/3158 - loss: 0.0156 - Perplexity: 1.0533 - 308ms/step\n",
      "step  400/3158 - loss: 0.0082 - Perplexity: 1.0349 - 306ms/step\n",
      "step  600/3158 - loss: 0.0099 - Perplexity: 1.0277 - 310ms/step\n",
      "step  800/3158 - loss: 0.0179 - Perplexity: 1.0242 - 311ms/step\n",
      "step 1000/3158 - loss: 0.0054 - Perplexity: 1.0215 - 313ms/step\n",
      "step 1200/3158 - loss: 0.0069 - Perplexity: 1.0199 - 316ms/step\n",
      "step 1400/3158 - loss: 0.0011 - Perplexity: 1.0185 - 315ms/step\n",
      "step 1600/3158 - loss: 0.0104 - Perplexity: 1.0174 - 315ms/step\n",
      "step 1800/3158 - loss: 0.0073 - Perplexity: 1.0167 - 315ms/step\n",
      "step 2000/3158 - loss: 0.0283 - Perplexity: 1.0159 - 316ms/step\n",
      "step 2200/3158 - loss: 0.0074 - Perplexity: 1.0154 - 317ms/step\n",
      "step 2400/3158 - loss: 0.0205 - Perplexity: 1.0150 - 316ms/step\n",
      "step 2600/3158 - loss: 0.0234 - Perplexity: 1.0145 - 316ms/step\n",
      "step 2800/3158 - loss: 0.0060 - Perplexity: 1.0142 - 316ms/step\n",
      "step 3000/3158 - loss: 0.0121 - Perplexity: 1.0140 - 315ms/step\n",
      "step 3158/3158 - loss: 9.3810e-04 - Perplexity: 1.0137 - 315ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Epoch 2/2\n",
      "step  200/3158 - loss: 0.0042 - Perplexity: 1.0078 - 309ms/step\n",
      "step  400/3158 - loss: 1.5228e-04 - Perplexity: 1.0071 - 307ms/step\n",
      "step  600/3158 - loss: 3.9863e-04 - Perplexity: 1.0066 - 312ms/step\n",
      "step  800/3158 - loss: 0.0051 - Perplexity: 1.0064 - 312ms/step\n",
      "step 1000/3158 - loss: 0.0011 - Perplexity: 1.0060 - 314ms/step\n",
      "step 1200/3158 - loss: 0.0026 - Perplexity: 1.0060 - 316ms/step\n",
      "step 1400/3158 - loss: 4.1019e-04 - Perplexity: 1.0058 - 315ms/step\n",
      "step 1600/3158 - loss: 4.5786e-04 - Perplexity: 1.0057 - 315ms/step\n",
      "step 1800/3158 - loss: 0.0026 - Perplexity: 1.0057 - 315ms/step\n",
      "step 2000/3158 - loss: 0.0101 - Perplexity: 1.0056 - 316ms/step\n",
      "step 2200/3158 - loss: 0.0108 - Perplexity: 1.0056 - 316ms/step\n",
      "step 2400/3158 - loss: 0.0085 - Perplexity: 1.0056 - 316ms/step\n",
      "step 2600/3158 - loss: 0.0147 - Perplexity: 1.0055 - 316ms/step\n",
      "step 2800/3158 - loss: 0.0222 - Perplexity: 1.0056 - 315ms/step\n",
      "step 3000/3158 - loss: 0.0036 - Perplexity: 1.0056 - 315ms/step\n",
      "step 3158/3158 - loss: 1.7115e-04 - Perplexity: 1.0055 - 315ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\r\n",
    "model.fit(train_dataloader, batch_size=batch_size, epochs=epochs, save_freq=5, save_dir='./checkpoints', log_freq=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-23 14:06:32,786] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n"
     ]
    }
   ],
   "source": [
    "import re\r\n",
    "# 导入预训练模型\r\n",
    "checkpoint_path = './checkpoints/final'  # 填写预训练模型的保存路径\r\n",
    "\r\n",
    "model = SkepForTokenClassification.from_pretrained('skep_ernie_1.0_large_ch', num_classes=3)\r\n",
    "input = InputSpec((-1, -1), dtype='int64', name='input')\r\n",
    "model = paddle.Model(model, [input])\r\n",
    "model.load(checkpoint_path)\r\n",
    "\r\n",
    "# 导入测试集\r\n",
    "test_dataloader = get_data_loader(data_dict[data_path_name]['test'], tokenizer, batch_size, max_len, for_test=True)\r\n",
    "# 预测保存\r\n",
    "\r\n",
    "save_file = {'cotebd': './COTE-BD.tsv', 'cotedp': './COTE-DP.tsv', 'cotemfw': './COTE-MFW.tsv'}\r\n",
    "predicts = []\r\n",
    "input_ids = []\r\n",
    "for batch in test_dataloader:\r\n",
    "    predict = model.predict_batch(batch)\r\n",
    "    predicts += predict[0].argmax(axis=-1).tolist()\r\n",
    "    input_ids += batch.numpy().tolist()\r\n",
    "\r\n",
    "# 先找到B所在的位置，即标号为0的位置，然后顺着该位置一直找到所有的I，即标号为1，即为所得。\r\n",
    "def find_entity(prediction, input_ids):\r\n",
    "    entity = []\r\n",
    "    entity_ids = []\r\n",
    "    for index, idx in enumerate(prediction):\r\n",
    "        if idx == label_list['B']:\r\n",
    "            entity_ids = [input_ids[index]]\r\n",
    "        elif idx == label_list['I']:\r\n",
    "            if entity_ids:\r\n",
    "                entity_ids.append(input_ids[index])\r\n",
    "        elif idx == label_list['O']:\r\n",
    "            if entity_ids:\r\n",
    "                entity.append(''.join(tokenizer.convert_ids_to_tokens(entity_ids)))\r\n",
    "                entity_ids = []\r\n",
    "    return entity\r\n",
    "\r\n",
    "with open(save_file[data_path_name], 'w', encoding='utf8') as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for idx, sample in enumerate(data_dict[data_path_name]['test']):\r\n",
    "        qid = sample.split('\\t')[0]\r\n",
    "        entity = find_entity(predicts[idx], input_ids[idx])\r\n",
    "        entity = list(set(entity)) # 去重\r\n",
    "        entity = [re.sub('##', '', i) for i in entity]  # 去除英文编码时的特殊字符\r\n",
    "        entity = [re.sub('[UNK]', '', i)for i in entity] # 去除未知符号 \r\n",
    "        f.write(qid + '\\t' + '\\x01'.join(entity) + '\\n')\r\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "以上实现基于PaddleNLP，开源不易，希望大家多多支持~ \n",
    "\n",
    "**记得给[PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)点个小小的Star⭐，及时跟踪最新消息和功能哦**\n",
    "\n",
    "GitHub地址：[https://github.com/PaddlePaddle/PaddleNLP](https://github.com/PaddlePaddle/PaddleNLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
